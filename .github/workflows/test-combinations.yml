name: Test Service Combinations

on:
  push:
    branches: [ develop ]
  pull_request:
    types: [ ready_for_review ]
  workflow_dispatch:

jobs:
  test-data-storage:
    name: Test Data Storage Layer
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup environment
        run: |
          cat > .env <<EOF
          POSTGRES_USER=admin
          POSTGRES_PASSWORD=admin123
          MINIO_ACCESS_KEY=minioadmin
          MINIO_SECRET_KEY=minioadmin123
          EOF
          
          mkdir -p scripts
          cat > scripts/init-postgres.sh <<'EOF'
          #!/bin/bash
          set -e
          psql -v ON_ERROR_STOP=1 --username "$POSTGRES_USER" <<-EOSQL
              CREATE DATABASE hive_metastore;
          EOSQL
          EOF
          chmod +x scripts/init-postgres.sh
      
      - name: Start storage services
        run: |
          docker compose \
            -f docker-compose.base.yml \
            -f docker-compose.postgres.yml \
            -f docker-compose.minio.yml \
            up -d
      
      - name: Test storage integration
        run: |
          # Wait for services
          timeout 60 bash -c 'until docker exec data-hub-postgres pg_isready -U admin; do sleep 2; done'
          timeout 60 bash -c 'until curl -f http://localhost:9000/minio/health/live; do sleep 2; done'
          
          # Test cross-service operations
          echo "Testing storage layer integration..."
          
          # Create test data in PostgreSQL
          docker exec data-hub-postgres psql -U admin -d hive_metastore -c \
            "CREATE TABLE test_metadata (id SERIAL PRIMARY KEY, bucket VARCHAR(50));"
          
          # Insert MinIO bucket reference
          docker exec data-hub-postgres psql -U admin -d hive_metastore -c \
            "INSERT INTO test_metadata (bucket) VALUES ('warehouse');"
          
          # Verify data
          docker exec data-hub-postgres psql -U admin -d hive_metastore -c \
            "SELECT * FROM test_metadata;" | grep warehouse
      
      - name: Cleanup
        if: always()
        run: |
          docker compose \
            -f docker-compose.base.yml \
            -f docker-compose.postgres.yml \
            -f docker-compose.minio.yml \
            down -v

  test-compute-layer:
    name: Test Compute Layer
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup environment
        run: |
          cat > .env <<EOF
          MINIO_ACCESS_KEY=minioadmin
          MINIO_SECRET_KEY=minioadmin123
          SPARK_MODE=cluster
          EOF
          
          mkdir -p config/spark jars
          cat > config/spark/spark-defaults.conf <<EOF
          spark.master spark://spark-master:7077
          EOF
          touch jars/{iceberg-spark-runtime,aws-java-sdk-bundle,hadoop-aws}.jar
      
      - name: Start compute services
        run: |
          docker compose \
            -f docker-compose.base.yml \
            -f docker-compose.minio.yml \
            -f docker-compose.spark.yml \
            up -d
      
      - name: Test compute integration
        run: |
          # Wait for services
          timeout 60 bash -c 'until curl -f http://localhost:9000/minio/health/live; do sleep 2; done'
          timeout 90 bash -c 'until curl -f http://localhost:8080; do sleep 3; done'
          
          # Wait for workers
          sleep 10
          
          # Test Spark with MinIO
          docker exec data-hub-spark-master python3 -c "
          from pyspark.sql import SparkSession
          spark = SparkSession.builder \
              .appName('TestMinIOIntegration') \
              .master('spark://spark-master:7077') \
              .config('spark.hadoop.fs.s3a.endpoint', 'http://minio:9000') \
              .config('spark.hadoop.fs.s3a.access.key', 'minioadmin') \
              .config('spark.hadoop.fs.s3a.secret.key', 'minioadmin123') \
              .config('spark.hadoop.fs.s3a.path.style.access', 'true') \
              .getOrCreate()
          
          # Create test DataFrame
          df = spark.range(100)
          
          # This would write to MinIO in a real scenario
          # For testing, we just verify the configuration works
          print('Spark-MinIO integration configured successfully')
          
          spark.stop()
          "
      
      - name: Cleanup
        if: always()
        run: |
          docker compose \
            -f docker-compose.base.yml \
            -f docker-compose.minio.yml \
            -f docker-compose.spark.yml \
            down -v

  test-orchestration-layer:
    name: Test Orchestration Layer
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup environment
        run: |
          cat > .env <<EOF
          POSTGRES_USER=admin
          POSTGRES_PASSWORD=admin123
          AIRFLOW_UID=50000
          AIRFLOW_USER=admin
          AIRFLOW_PASSWORD=admin
          AIRFLOW_FERNET_KEY=12345678901234567890123456789012345678901234
          AIRFLOW_SECRET_KEY=secret12345678901234567890123456789012345678901234
          SPARK_MODE=cluster
          EOF
          
          mkdir -p config/spark dags scripts jars
          
          cat > scripts/init-postgres.sh <<'EOF'
          #!/bin/bash
          set -e
          psql -v ON_ERROR_STOP=1 --username "$POSTGRES_USER" <<-EOSQL
              CREATE DATABASE airflow;
          EOSQL
          EOF
          chmod +x scripts/init-postgres.sh
          
          cat > config/spark/spark-defaults.conf <<EOF
          spark.master spark://spark-master:7077
          EOF
          
          touch jars/{iceberg-spark-runtime,aws-java-sdk-bundle,hadoop-aws}.jar
          
          cat > dags/test_orchestration.py <<'EOF'
          from airflow import DAG
          from airflow.operators.bash import BashOperator
          from datetime import datetime
          
          dag = DAG('test_orchestration', 
                    start_date=datetime(2024, 1, 1),
                    schedule_interval=None)
          
          task = BashOperator(
              task_id='test_spark_connection',
              bash_command='echo "Testing Spark connection"',
              dag=dag
          )
          EOF
      
      - name: Start orchestration services
        run: |
          docker compose \
            -f docker-compose.base.yml \
            -f docker-compose.postgres.yml \
            -f docker-compose.spark.yml \
            -f docker-compose.airflow.yml \
            up -d
      
      - name: Test orchestration integration
        run: |
          # Wait for services
          timeout 60 bash -c 'until docker exec data-hub-postgres pg_isready -U admin; do sleep 2; done'
          timeout 90 bash -c 'until curl -f http://localhost:8080; do sleep 3; done'
          timeout 180 bash -c 'until curl -f http://localhost:8088/health; do sleep 5; done'
          
          # Test Airflow-Spark integration
          docker exec data-hub-airflow-scheduler airflow dags list | grep test_orchestration
          docker exec data-hub-airflow-scheduler airflow dags test test_orchestration 2024-01-01
          
          # Verify Spark workers are accessible from Airflow
          curl -s http://localhost:8080/json/ | python3 -c "
          import sys, json
          data = json.load(sys.stdin)
          assert len(data.get('workers', [])) >= 2, 'Spark workers not registered'
          print('Orchestration layer integration OK')
          "
      
      - name: Cleanup
        if: always()
        run: |
          docker compose \
            -f docker-compose.base.yml \
            -f docker-compose.postgres.yml \
            -f docker-compose.spark.yml \
            -f docker-compose.airflow.yml \
            down -v